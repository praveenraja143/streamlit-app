import os
import streamlit as st
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from dotenv import load_dotenv
from PyPDF2 import PdfReader
from huggingface_hub import InferenceClient
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from PIL import Image

# âœ… Load .env
load_dotenv()
HF_TOKEN = os.getenv("HF_TOKEN")

# ğŸ” Check token
if HF_TOKEN is None:
    st.error("âŒ HF_TOKEN not found. Please check your .env file.")
    st.stop()

# âœ… Hugging Face clients
image_client = InferenceClient(provider="nebius", api_key=HF_TOKEN)
text_client = InferenceClient(provider="novita", api_key=HF_TOKEN)

# âœ… Streamlit page setup
st.set_page_config(page_title="ğŸ§  Multi-AI Toolkit", layout="centered")
st.title("ğŸ§ Praveenraja,Anas,Ranjith-AI App: Image + Chat + PDF + CSV Analyzer")

# âœ… Tabs setup
tab1, tab2, tab3, tab4 = st.tabs([
    "ğŸ–¼ï¸ Image Generator", 
    "ğŸ’¬ LLaMA Chat", 
    "ğŸ“„ PDF Summarizer", 
    "ğŸ“Š CSV Analyzer + Model"
])

# ----------------------
# ğŸ–¼ï¸ Tab 1: Image Generator
# ----------------------
with tab1:
    st.header("ğŸ–¼ï¸ Generate Image from Text")
    prompt = st.text_input("ğŸ¨ Enter image prompt:", "Astronaut riding a horse")

    if st.button("ğŸ¨ Generate Image"):
        with st.spinner("ğŸ§  Creating image..."):
            try:
                image = image_client.text_to_image(prompt=prompt, model="black-forest-labs/FLUX.1-dev")
                img_path = os.path.join(os.getcwd(), "generated_image.png")
                image.save(img_path)
                st.image(image, caption="Generated Image", use_container_width=True)
                with open(img_path, "rb") as f:
                    st.download_button("ğŸ“¥ Download Image", f, "generated_image.png", "image/png")
            except Exception as e:
                st.error(f"âŒ Error: {e}")

# ----------------------
# ğŸ’¬ Tab 2: LLaMA Chat
# ----------------------
with tab2:
    st.header("ğŸ’¬ Chat with LLaMA 3.1 8B")
    user_input = st.text_area("âœï¸ Ask a question:", placeholder="e.g., What is the capital of France?")

    if st.button("ğŸš€ Get LLaMA Response"):
        if user_input.strip():
            with st.spinner("ğŸ¤– Thinking..."):
                try:
                    res = text_client.chat.completions.create(
                        model="meta-llama/Llama-3.1-8B-Instruct",
                        messages=[{"role": "user", "content": user_input}],
                    )
                    reply = res.choices[0].message["content"]
                    st.subheader("ğŸ§¾ Response:")
                    st.write(reply)

                    with open("llama_output.txt", "w", encoding="utf-8") as f:
                        f.write(reply)
                    with open("llama_output.txt", "rb") as f:
                        st.download_button("ğŸ“¥ Download .txt", f, "llama_output.txt", "text/plain")
                except Exception as e:
                    st.error(f"âŒ Chat error: {e}")
        else:
            st.warning("âš ï¸ Please enter a question.")

# ----------------------
# ğŸ“„ Tab 3: PDF Summarizer
# ----------------------
with tab3:
    st.header("ğŸ“„ Summarize a PDF File")
    uploaded_pdf = st.file_uploader("ğŸ“¤ Upload a PDF file", type="pdf")

    if uploaded_pdf:
        try:
            reader = PdfReader(uploaded_pdf)
            pdf_text = ""
            for page in reader.pages:
                pdf_text += page.extract_text()

            st.subheader("ğŸ“š Preview (First 1000 characters):")
            st.text(pdf_text[:1000] + "...")

            if st.button("ğŸ§  Summarize PDF"):
                with st.spinner("ğŸ“ Summarizing..."):
                    try:
                        summary_prompt = f"Summarize the following PDF content:\n{pdf_text[:3000]}"
                        res = text_client.chat.completions.create(
                            model="meta-llama/Llama-3.1-8B-Instruct",
                            messages=[{"role": "user", "content": summary_prompt}],
                        )
                        summary = res.choices[0].message["content"]

                        st.subheader("ğŸ“ Summary:")
                        st.write(summary)

                        with open("pdf_summary.txt", "w", encoding="utf-8") as f:
                            f.write(summary)
                        with open("pdf_summary.txt", "rb") as f:
                            st.download_button("ğŸ“¥ Download Summary", f, "pdf_summary.txt", "text/plain")
                    except Exception as e:
                        st.error(f"âŒ Error summarizing: {e}")
        except Exception as e:
            st.error(f"âŒ Could not read PDF: {e}")

# ----------------------
# ğŸ“Š Tab 4: CSV Analyzer + Model
# ----------------------
with tab4:
    st.header("ğŸ“Š CSV Analyzer & Model Prediction")
    csv_file = st.file_uploader("ğŸ“‚ Upload a CSV file", type="csv")

    if csv_file:
        try:
            df = pd.read_csv(csv_file)
            st.subheader("ğŸ” Dataset Preview:")
            st.write(df.head())

            st.subheader("ğŸ“ˆ Data Summary:")
            st.write(df.describe())
            st.write(f"ğŸ§¾ Rows: {df.shape[0]}, Columns: {df.shape[1]}")

            st.subheader("ğŸ“Š Correlation Heatmap:")
            fig, ax = plt.subplots(figsize=(10, 6))
            sns.heatmap(df.corr(numeric_only=True), annot=True, cmap="coolwarm", ax=ax)
            st.pyplot(fig)

            st.subheader("ğŸ¯ Select Target Column for ML")
            target_column = st.selectbox("Choose column to predict:", df.columns)

            if target_column:
                X = df.drop(columns=[target_column])
                y = df[target_column]
                X = pd.get_dummies(X)

                X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

                model = RandomForestClassifier()
                model.fit(X_train, y_train)

                y_pred = model.predict(X_test)
                acc = accuracy_score(y_test, y_pred)

                st.subheader("ğŸ“ˆ Model Results")
                st.write(f"âœ… Accuracy: **{acc * 100:.2f}%**")
                st.markdown(f"""
                **Model Summary**
                - ğŸ“Š Features used: {len(X.columns)}
                - ğŸ¯ Target: `{target_column}`
                - ğŸ¤– Model: RandomForestClassifier
                - âœ… Accuracy: `{acc * 100:.2f}%`
                """)
        except Exception as e:
            st.error(f"âŒ Error processing CSV: {e}")
